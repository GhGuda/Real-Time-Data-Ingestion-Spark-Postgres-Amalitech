"""
Spark Structured Streaming for Real-Time E-commerce Events

This script reads real-time JSON e-commerce events generated by the
data generator, applies a predefined schema, and processes the data
using Spark Structured Streaming.
"""

import logging
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, 
    IntegerType, DoubleType, TimestampType)

from configs.logger_config import setup_logger
setup_logger()

# -------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------
setup_logger()
logger = logging.getLogger(__name__)

# -------------------------------------------------------------------
# Schema Definition
# -------------------------------------------------------------------
event_schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("event", StringType(), True),
    StructField("user", StringType(), True),
    StructField("product", StringType(), True),
    StructField("price", DoubleType(), True),
    StructField("timestamp", TimestampType(), True)
])


# -------------------------------------------------------------------
# Spark Session
# -------------------------------------------------------------------
spark = (
    SparkSession.builder
    .appName("SparkStreamingToPostgres")
    .config("spark.sql.warehouse.dir", "file:/C:/tmp/spark-warehouse")
    .config("spark.hadoop.fs.defaultFS", "file:///")
    .getOrCreate()
)


logger.info("Spark session created")


# -------------------------------------------------------------------
# Read Streaming Data
# -------------------------------------------------------------------
input_path = "file:///C:/Users/MelchizedekTettehNar/OneDrive - AmaliTech gGmbH/Desktop/Real-Time-Data-Ingestion-Spark-Postgres/data/input"

try:
    streaming_df = (
        spark.readStream
        .format("json")
        .load(input_path)
    )

    logging.info("Streaming source initialized successfully")

except Exception as e:
    logging.error(f"Failed to read streaming data: {e}")
    spark.stop()
    raise


# -------------------------------------------------------------------
# Output Stream (Console for validation)
# -------------------------------------------------------------------
query = streaming_df.writeStream \
    .format("console") \
    .outputMode("append") \
    .option("truncate", "false") \
    .start()

logger.info("Streaming query started")

query.awaitTermination()

 