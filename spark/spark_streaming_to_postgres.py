"""
Spark Structured Streaming for Real-Time E-commerce Events

This script reads real-time JSON e-commerce events generated by the
data generator, applies a predefined schema, and processes the data
using Spark Structured Streaming.
"""

import logging
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, 
    IntegerType, DoubleType)

from configs.logger_config import setup_logger
setup_logger()

# -------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------
setup_logger()
logger = logging.getLogger(__name__)

# -------------------------------------------------------------------
# Schema Definition
# -------------------------------------------------------------------
event_schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("event", StringType(), True),
    StructField("user", StringType(), True),
    StructField("product", StringType(), True),
    StructField("price", DoubleType(), True),
    StructField("timestamp", StringType(), True)
])



def main():
    try:
        # -------------------------------------------------------------------
        # Spark Session
        # -------------------------------------------------------------------
        spark = (
            SparkSession.builder
            .appName("DockerSparkStreaming")
            .getOrCreate()
        )

        # -------------------------------------------------------------------
        # Read Streaming Data
        # -------------------------------------------------------------------
        df = (
            spark.readStream
            .schema(event_schema)
            .json("/data/input")
        )

        # -------------------------------------------------------------------
        # Output Stream (Console for validation)
        # -------------------------------------------------------------------
        query = (
            df.writeStream
            .format("console")
            .outputMode("append")
            .option("truncate", "false")
            .start()
        )


        logger.info("Streaming query running")
        query.awaitTermination()
 
    except Exception as e:
        logging.error(f"Failed to read streaming data: {e}")
        spark.stop()
        raise

if __name__ == "__main__":
    main()
